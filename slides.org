#+TITLE: Graphics Rendering as (sort of) Functional Programming
#+DATE: 2018-06-12
#+AUTHOR: Chris Hodapp

* Introduction
  - Graphics were what first interested me in computers.
    - Dazzle
    - DOOM
    - POV-Ray, PolyRay
    - Toy Story?
  - ...and then sort of what led me to FP.
    - Generating images pixel-by-pixel
    - C, SDL, framebuffers
    - Shaders

#+BEGIN_NOTES
  - Can demo Dazzle here in DOSBOX
  - XaOS?
  - Doom screenshot?
  - The FP stuff: Lisp for macros, Scala for an EDSL
#+END_NOTES

** Motivation

   - Normal CincyFP focus:
     - functional paradigm in contrast to imperative
     - best practices & design for software
   - Focus in this talk: Examples from graphics rendering that are
     from a different angle.
   - i.e. FP methods not as an alternative to imperative methods - but
     alternative to "static" data, by representing that data as an
     algorithm for generating it.
   - Parametric/procedural design broadly covers this

#+BEGIN_NOTES
#+END_NOTES

** Focus
   
   - Graphics shaders
     - /Realtime (i.e. video games)/
       - OpenGL Shading Language (GLSL)
     - /Offline (i.e. production-grade movie CGI)/
       - [[https://github.com/imageworks/OpenShadingLanguage][Open Shading Language]] (OSL) from Sony
       - RenderMan Shading Language from Pixar (deprecated for OSL &
         C++)
   - 3D models as functional representations ([[https://en.wikipedia.org/wiki/Function_representation][F-Reps]])
     - a.k.a. implicit surfaces/isosurfaces

#+BEGIN_NOTES
   - Give good definition of both
   - Yes, OpenGL Shading Language and Open Shading Language are
     completely unrelated (aside from both being shading languages)
#+END_NOTES

* Graphics shaders
  
  - Family of domain-specific languages (typically C/Java-like with
    additional constraints)
  - Ultimately: for writing functions whose input is a point
    in 2D/3D space and whose output is a color value (RGB, RGBA)
    - Sort of a generalization of an image, which also maps points to
      color
  - [[https://github.com/patriciogonzalezvivo/glslViewer][glslViewer]] & [[https://thebookofshaders.com/][Book of Shaders]]

#+BEGIN_NOTES
   - I've sort of glossed over that the inputs also include things
     like camera positions, but they don't really change the fundamentals
   - TODO: Does glslViewer link belong here or later?
#+END_NOTES

** Shader functions

   - Minus a few exceptions (time, other parameters, image inputs),
     these are pure functions that run separately on each element -
     typically a pixel or texel
     - No side effects.
     - No memory across runs.
     - No communication.

#+BEGIN_NOTES
#+END_NOTES

** Shader functions

   - Why: Shaders must be able to run deterministically and in
     parallel across many elements (typically pixels or texels).
     - Movie CGI: Distributed across whole cluster
     - Video games: Run in parallel on GPU
   - Almost all of you now have several devices with dedicated silicon
     for handling shaders and a compiler for GLSL

#+BEGIN_NOTES
  - This is also why CUDA & OpenCL exist
  - Short, mostly-inaccurate history:
    - Custom render engines in software (Doom, Quake, Descent)
    - Hardware rendering for pushing triangles
    - Fixed hardware pipelines...
    - More flexible hardware pipelines......
    - ?????
    - Now: Almost every computer, phone, tablet has a stream processor
      orders of magnitudes faster than the CPU, and programmable in
      (at minimum) OpenGL Shading Language
#+END_NOTES

** Perlin noise

    - [[https://dl.acm.org/citation.cfm?id=325247][Ken Perlin: An image synthesizer (1985)]]
    - Well-behaved, deterministic, fast randomness

 #+BEGIN_NOTES
   - Emerged right around the same time as REYES & RenderMan Shading
     Language
   - TODO: What can I show with Perlin noise?
 #+END_NOTES

** Summary

   - Technical constraints led to FP approaches - but these approaches
     brought other benefits:
     - Very lightweight representation (versus stored images), even
       for complex "natural" textures
     - Unlimited (ish) resolution
     - Independence from underlying renderer

 #+BEGIN_NOTES
   - I am cheating slightly since some of these benefits are from
     procedural expression, and that it's FP-ish is irrelevant
 #+END_NOTES

* Raytracing

  - As name implies: traces camera rays from each pixel into the
    scene.
  - Renders anything with a ray intersection formula.
  - Handles things like reflection, refraction, translucency with no
    faking required.

#+BEGIN_NOTES
  - A lot of the clever use of shaders in RenderMan was to work around
    limitations of scanline rendering (as REYES was heavily oriented
    around, rather than raytracing).
  - However, shaders evolved to handle all sorts of less "faked"
    lighting
  - TODO: Visualizations? POV-Ray?
#+END_NOTES

** POV-Ray, Clojure, Twitter, and Heroku?

   - [[https://twitter.com/nailpolishbot][Nail polish bot]]

** Limitations on shapes/primitives

   - What if you don't have a ray intersection formula?
     - Implicit surfaces, isosurfaces, level surfaces
     - Displacement
     - Fractals
   - Turn it to a triangle mesh and use that?  (Use marching cubes algorithm?)
#+BEGIN_NOTES
   - Next slide - raymarching - is one answer to this
#+END_NOTES

* Raymarching

  - Similar to raytracing - but handles cases where it must approximate.
    - surfaces that are a pain (no analytic intersection formula)
    - things with no surfaces (e.g. volumes with varying density)
  - [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.438.4926&rep=rep1&type=pdf][Ken Perlin again: Hypertexture (1989)]]
  - [[https://www.researchgate.net/publication/234777691_Ray_tracing_deterministic_3-D_fractals][John C. Hart: Ray tracing deterministic 3D fractals (1989)]]
  - [[http://gigavoxels.inrialpes.fr/][GigaVoxels]]

#+BEGIN_NOTES
  - TODO: Show some examples of these (maybe both fractals and
    volumes)
#+END_NOTES

** Sphere tracing / distance estimation

  - Ray marching from a *distance field* or *distance estimator*
    function
    - For any point in 3D space, returns a lower bound on the nearest
      distance to the surface/scene/object/whatever
    - Equivalently: For every 3D point /p/, gives the radius of a
      sphere centered at /p/ for which none of the surface is inside
      the sphere. ("Unbounding volumes")
  - [[http://mathinfo.univ-reims.fr/IMG/pdf/hart94sphere.pdf][Sphere Tracing: A Geometric Method for the Antialiased Ray Tracing of Implicit Surfaces]] (John C. Hart)
  - Íñigo Quílez: [[http://www.iquilezles.org/www/material/nvscene2008/rwwtt.pdf][Rendering Worlds with Two Triangles]]

#+BEGIN_NOTES
  - Link to some of IQ's shadertoy or pouet examples
  - Note on Lipschitz continuity
  - Terminology is sort of awful. Hart's sphere tracing paper uses
    "distance bound" for a function giving lower distance bound, and
    "distance function" for exact distance.  His older fractal paper
    uses "distance estimate" rather than "distance bound", I think.
#+END_NOTES

** Why/how?
   - Because you can do it in realtime completely in a GPU shader
   - Blah blah blah mathematical elegance
   - It's an amazingly flexible representation
     - Domain transformations (see: [[http://iquilezles.org/www/articles/distfunctions/distfunctions.htm][Modeling with distance functions]])
     - CSG
   - Because it's cool, mostly

#+BEGIN_NOTES
  - Link to some of IQ's shadertoy or pouet examples
#+END_NOTES

** Implicit Surfaces, Isosurfaces, F-Reps

   - [[https://en.wikipedia.org/wiki/Function_representation][F-Reps]] = implicit surfaces = isosurfaces
   - Signed distance estimates/bounds are automatically F-Reps
   - SDFs (signed distance functions/fields) are automatically signed
     distance estimates (thus, F-Reps)
   - Sorry, I didn't invent this terminology

#+BEGIN_NOTES
  - Give equation and maybe a sphere example?
  - Maybe split to 2 slides
#+END_NOTES

** The point...

   - Raytracing relies on either:
     - very limited parametric shapes (as it requires ray intersection
       formulas),
     - Dense triangle meshes that are approximate
   - Neither one is a particularly "functional" approach.

#+BEGIN_NOTES
  - This is oversimplifying a little
  - Meshes are just sort of flat data
  - Not many transformations work meaningfully on the parametric
    shapes (without simply facetizing them)
#+END_NOTES

** The point...

   - F-Reps bypass all of this
   - Arbitrary domain transformations

#+BEGIN_NOTES
  - Explain/show what domain transformations are
#+END_NOTES


* Other Links
  - [[https://github.com/patriciogonzalezvivo/glslViewer][glslViewer]] & [[https://thebookofshaders.com/][Book of Shaders]]
  - Literally everything from from [[http://iquilezles.org/www/index.htm][Íñigo Quílez]]
  - [[http://blog.hvidtfeldts.net/index.php/2011/06/distance-estimated-3d-fractals-part-i/][Syntopia: Distance Estimated 3D Fractals]] & [[https://syntopia.github.io/Fragmentarium/][Fragmentarium]] / [[https://github.com/3Dickulus/FragM][FragM]]
  - ShaderToy
  - https://hodapp87.github.io/cs6460_project/

* Final notes
  - Twitter: @hodapp87
  - GitHub: https://github.com/hodapp87
  - Slides proudly generated with Emacs, [[https://github.com/yjwen/org-reveal][org-reveal]], and [[https://revealjs.com/][reveal.js]].

* Slush Bucket
** Movies & 3D CGI

   - Various practical problems:
     - Raytracing is slow
     - Scanline rendering is faster, but looks bad
     - Image are sort of bulky and inelegant

 #+BEGIN_NOTES
   - Have a good definition/example of scanline rendering
 #+END_NOTES

*** Pixar & RenderMan

    - Facetize everything to triangles < 1 pixel
    - RenderMan Shading Language
    - Pre-compute & pre-shade
    - Distributes easily across a cluster
    - [[https://www.youtube.com/watch?v=ffIZSAZRzDA][Tin Toy (1988)]]: First CGI film to win Oscar
    - Toy Story (1995): First full-length CGI film

** The point...

- Both sort of replaced *data* with *functions*.
  - Instead of triangle meshes: basic shapes + transformations
  - Instead of image maps: compositions of noise functions
- Both used functional approaches as practical solutions.

** Raytracing limitations: Lighting

   - Simple raytracing by itself handles only *direct illumination*.
   - [[https://en.wikipedia.org/wiki/Global_illumination][Global illumination]]
     - Numerical approximations of the [[https://en.wikipedia.org/wiki/Rendering_equation][rendering equation]]
       - [[https://en.wikipedia.org/wiki/Unbiased_rendering][Unbiased]] vs. biased renderers
     - Path tracing
     - Metropolis Light Transport
     - Photon mapping
     - [[https://en.wikipedia.org/wiki/Radiosity_(computer_graphics)][Radiosity]]
     - [[https://en.wikipedia.org/wiki/Ambient_occlusion][Ambient occlusion]]
   - This is a "hard problem"(tm) and I'm ignoring it here.

 #+BEGIN_NOTES
   - How necessary is this slide?
   - Give real-world example of why this is needed
   - Explain why ray tracing by itself doesn't cover this
   - Show examples of some (e.g. AO)
   - http://www.yafaray.org/documentation/userguide/lightingmethods
 #+END_NOTES


** Modern Day
   - Raytracing is now much more common in movie CGI
   - Intel and NVidia are also pushing it for realtime rendering
   - RenderMan Shading Language is now deprecated
   - Sony Pictures ImageWorks: [[https://github.com/imageworks/OpenShadingLanguage][OSL (Open Shading Language)]]
     - [[http://www.blender.org/][Blender]] implements OSL

