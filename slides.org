#+TITLE: Shaders = Lazy Images, F-Reps = Lazy Geometry.
#+DATE: 2018-06-12
#+AUTHOR: Chris Hodapp

* Introduction
  - Graphics were what first interested me in computers.
    - Dazzle
    - DOOM
    - POV-Ray, PolyRay
    - Toy Story?
  - ...and then sort of what led me to FP.
    - Generating images pixel-by-pixel
    - C, SDL, framebuffers
    - Shaders

#+BEGIN_NOTES
  - Can demo Dazzle here in DOSBOX
  - XaOS?
  - Doom screenshot?
  - The FP stuff: Lisp for macros, Scala for an EDSL
#+END_NOTES

** Motivation

   - Normal CincyFP focus:
     - functional paradigm in contrast to imperative
     - best practices & design for software
   - Focus in this talk: Examples from graphics rendering that are
     from a different angle.
   - i.e. FP methods not as an alternative to imperative methods - but
     alternative to "static" data, by representing that data as an
     algorithm for generating it.

#+BEGIN_NOTES
#+END_NOTES

** Focus
   
   - Graphics shaders
     - /Realtime (i.e. video games)/
       - OpenGL Shading Language (GLSL)
     - /Offline (i.e. production-grade movie CGI)/
       - [[https://github.com/imageworks/OpenShadingLanguage][Open Shading Language]] (OSL) from Sony
       - RenderMan Shading Language from Pixar (deprecated for OSL &
         C++)
   - 3D models as functional representations ([[https://en.wikipedia.org/wiki/Function_representation][F-Reps]])
     - a.k.a. implicit surfaces/isosurfaces

#+BEGIN_NOTES
   - Give good definition of both
   - Yes, OpenGL Shading Language and Open Shading Language are
     completely unrelated (aside from both being shading languages)
#+END_NOTES

* Graphics shaders
  
  - Family of domain-specific languages (typically C/Java-like with
    additional constraints)
  - Ultimately: for writing functions whose input is a point
    in 2D/3D space and whose output is a color value (RGB, RGBA)
    - Sort of a generalization of an image, which also maps points to
      color
  - [[https://github.com/patriciogonzalezvivo/glslViewer][glslViewer]] & [[https://thebookofshaders.com/][Book of Shaders]]

#+BEGIN_NOTES
   - I've sort of glossed over that the inputs also include things
     like camera positions, but they don't really change the fundamentals
   - TODO: Does glslViewer link belong here or later?
#+END_NOTES

** Shader functions

   - Minus a few exceptions (time, other parameters, image inputs),
     these are pure functions that run separately on each element -
     typically a pixel or texel
     - No side effects.
     - No memory across runs.
     - No communication.

#+BEGIN_NOTES
   - TODO: Shader example from ~glslViewer bunny.frag bunny.vert bunny.pl~
   - Show diffuse shader, ambient, specular (if I can get it)
   - Shaders in "real" usage can be sort of arcane...
#+END_NOTES

** Shader functions

   - Why: Shaders must be able to run deterministically and in
     parallel across many elements (typically pixels or texels).
     - Movie CGI: Distributed across whole cluster
     - Video games: Run in parallel on GPU
   - Almost all of you now have several devices with dedicated silicon
     for handling shaders and a compiler for GLSL

#+BEGIN_NOTES
  - This is also why CUDA & OpenCL exist
  - Short, mostly-inaccurate history:
    - Custom render engines in software (Doom, Quake, Descent)
    - Hardware rendering for pushing triangles
    - Fixed hardware pipelines...
    - More flexible hardware pipelines......
    - ?????
    - Now: Almost every computer, phone, tablet has a stream processor
      orders of magnitudes faster than the CPU, and programmable in
      (at minimum) OpenGL Shading Language
#+END_NOTES

** Examples...
   [[https://upload.wikimedia.org/wikipedia/commons/6/6b/Phong_components_version_4.png]]

** Perlin noise

    - [[https://dl.acm.org/citation.cfm?id=325247][Ken Perlin: An image synthesizer (1985)]]
    - Well-behaved, deterministic, fast randomness

 #+BEGIN_NOTES
   - Emerged right around the same time as REYES & RenderMan Shading
     Language
   - TODO: What can I show with Perlin noise?
 #+END_NOTES

** Summary

   - Technical constraints led to FP approaches - but these approaches
     brought other benefits:
     - Very lightweight representation (versus stored images), even
       for complex "natural" textures
     - Can be sampled at arbitrary resolution
     - Independent from underlying renderer

 #+BEGIN_NOTES
   - I am cheating slightly since some of these benefits are from
     procedural expression, and that it's FP-ish is irrelevant
   - Hold all of these 2D transformations in mind
 #+END_NOTES

* Geometry, Shapes, Models

  - Representation depends heavily on requirements...
    - Is shape raytraced or scanline rendered?
    - Facetized to arbitrary precision? (e.g. RenderMan)
    - Numerically exact? (3D CAD/CAM)
    - Sparse vs. dense?  How compact must it be?
    - Did 3D measurements or simulations produce it?  (MRI, CT scan,
      laser scan, Finite Element Analysis)
    - What transformations should be easy?
    - Must it be a manifold with interior/exterior?

#+BEGIN_NOTES
  - Geometry & shaders don't have sharp boundaries.  In RenderMan and
    in more modern OpenGL, shaders influence geometry directly.
  - However, shaders evolved to handle all sorts of less "faked"
    lighting
  - TODO: Visualizations? POV-Ray?
#+END_NOTES

** Examples

   - Meshes, voxels
   - Point clouds
   - Analytic primitives, closed-form intersection formulas
   - NURBS
   - Isosurfaces, SDFs
   - [[http://gigavoxels.inrialpes.fr/][GigaVoxels]]

** Example: Triangle mesh

   https://upload.wikimedia.org/wikipedia/commons/f/fb/Dolphin_triangle_mesh.png

** Example: Point clouds

   [[https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif]]

** Example: Voxels

   https://upload.wikimedia.org/wikipedia/commons/4/47/Ribo-Voxels.png

** Example: NURBS

   https://upload.wikimedia.org/wikipedia/commons/e/ea/NURBS_3-D_surface.gif

** Raytracing

  - As name implies: traces camera rays from each pixel into the
    scene.
  - Renders anything with a ray intersection formula.
  - Handles things like reflection, refraction, translucency with no
    faking required.

#+BEGIN_NOTES
  - A lot of the clever use of shaders in RenderMan was to work around
    limitations of scanline rendering (as REYES was heavily oriented
    around, rather than raytracing).
  - However, shaders evolved to handle all sorts of less "faked"
    lighting
  - TODO: Visualizations? POV-Ray?
#+END_NOTES

** Raytracing

   https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Ray_trace_diagram.svg/875px-Ray_trace_diagram.svg.png

** POV-Ray, Clojure, Twitter, and Heroku?

   - [[https://twitter.com/nailpolishbot][Nail polish bot]]

** Limitations on shapes/primitives

   - What if you don't have a ray intersection formula?
     - Isosurfaces, SDFs
     - Displacement
     - Fractals
   - Turn it to a triangle mesh and use that?  (Use marching cubes algorithm?)
#+BEGIN_NOTES
   - Next slide - raymarching - is one answer to this
#+END_NOTES

* Raymarching & F-Reps

  - Raymarching is sort of like raytracing, but for where intersection
    must be iterative & approximate rather than analytical:
    - Surfaces that are a pain (no analytic intersection formula)
    - Things with no surfaces (e.g. volumes with varying density)
  - [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.438.4926&rep=rep1&type=pdf][Ken Perlin again: Hypertexture (1989)]]
  - [[https://www.researchgate.net/publication/234777691_Ray_tracing_deterministic_3-D_fractals][John C. Hart: Ray tracing deterministic 3D fractals (1989)]]

#+BEGIN_NOTES
  - TODO: Show some examples of these (maybe both fractals and
    volumes)
#+END_NOTES

** Isosurfaces

   - As far as we care: isosurfaces are functions $f(x,y,z)$
     representing a 3D surface in which, for 3D point $(x,y,z)$:
     - $f(x,y,z) > 0$ outside of surface
     - $f(x,y,z) < 0$ inside of surface
     - $f(x,y,z) = 0$ on surface
   - Simple example: A sphere of radius $R$ centered at $(0,0,0)$ is
     $$f(x,y,z) = (x^2+y^2+z^2)-R^2$$

** I'm sorry. I didn't name them.

   - *Isosurface = [[https://en.wikipedia.org/wiki/Function_representation][F-Rep]] = implicit surface = level surface*
   - *Signed distance estimate = signed distance bound = unbounding volume (?)*
     - Isosurface with more rules: $(x,y,z)$ is distance
       $\geq |f(x,y,z)|$ away from nearest point on surface
   - *SDF = signed distance field = signed distance function*
     - Signed distance bound (thus, isosurface also) with more rules:
       $(x,y,z)$ is distance $|f(x,y,z)|$ away from nearest point on
       surface.

** I'm sorry, part 2

   - Any [[https://en.wikipedia.org/wiki/Lipschitz_continuity][Lipschitz continuous]] isosurface can be turned to a signed
     distance bound, which is Left As An Exercise To the Reader(tm) or
     just go read [[http://mathinfo.univ-reims.fr/IMG/pdf/hart94sphere.pdf][Sphere Tracing: A Geometric Method for the
     Antialiased Ray Tracing of Implicit Surfaces]] by John C. Hart
   - Let's all just agree to ignore unsigned distance
     bounds/fields/functions/estimates because I've completely stopped
     caring at this point

** Why bother?
   - Because you can do it in realtime completely in a GPU shader
   - Blah blah blah mathematical elegance
   - Similar handy things as shaders, plus 3D stuff:
     - Domain transformations (see: [[http://iquilezles.org/www/articles/distfunctions/distfunctions.htm][Modeling with distance functions]])
     - CSG
   - Because it's cool, mostly

#+BEGIN_NOTES
  - Link to some of IQ's shadertoy or pouet examples
  - Show libfive examples
#+END_NOTES

** Sphere tracing / distance estimation

  - Ray marching from distance bounds/estimates
  - Íñigo Quílez: [[http://www.iquilezles.org/www/material/nvscene2008/rwwtt.pdf][Rendering Worlds with Two Triangles]]

#+BEGIN_NOTES
  - Link to some of IQ's shadertoy or pouet examples
#+END_NOTES

** The point...

   - Raytracing relies on either:
     - very limited parametric shapes (as it requires ray intersection
       formulas),
     - Dense triangle meshes that are approximate
   - Neither one is a particularly "functional" approach.

#+BEGIN_NOTES
  - This is oversimplifying a little
  - Meshes are just sort of flat data
  - Not many transformations work meaningfully on the parametric
    shapes (without simply facetizing them)
#+END_NOTES

** The point...

   - F-Reps bypass all of this
   - Arbitrary domain transformations

#+BEGIN_NOTES
  - Explain/show what domain transformations are
#+END_NOTES


* Other Links
  - [[https://github.com/patriciogonzalezvivo/glslViewer][glslViewer]] & [[https://thebookofshaders.com/][Book of Shaders]]
  - Literally everything from [[http://iquilezles.org/www/index.htm][Íñigo Quílez]]
  - [[http://blog.hvidtfeldts.net/index.php/2011/06/distance-estimated-3d-fractals-part-i/][Syntopia: Distance Estimated 3D Fractals]] & [[https://syntopia.github.io/Fragmentarium/][Fragmentarium]] / [[https://github.com/3Dickulus/FragM][FragM]]
  - ShaderToy
  - https://hodapp87.github.io/cs6460_project/

* Final notes
  - Twitter: @hodapp87
  - GitHub: https://github.com/hodapp87
  - Slides proudly generated with Emacs & [[https://github.com/yjwen/org-reveal][org-reveal]]

* Slush Bucket
** Movies & 3D CGI

   - Various practical problems:
     - Raytracing is slow
     - Scanline rendering is faster, but looks bad
     - Image are sort of bulky and inelegant

 #+BEGIN_NOTES
   - Have a good definition/example of scanline rendering
 #+END_NOTES

*** Pixar & RenderMan

    - Facetize everything to triangles < 1 pixel
    - RenderMan Shading Language
    - Pre-compute & pre-shade
    - Distributes easily across a cluster
    - [[https://www.youtube.com/watch?v=ffIZSAZRzDA][Tin Toy (1988)]]: First CGI film to win Oscar
    - Toy Story (1995): First full-length CGI film

** The point...

- Both sort of replaced *data* with *functions*.
  - Instead of triangle meshes: basic shapes + transformations
  - Instead of image maps: compositions of noise functions
- Both used functional approaches as practical solutions.

** Raytracing limitations: Lighting

   - Simple raytracing by itself handles only *direct illumination*.
   - [[https://en.wikipedia.org/wiki/Global_illumination][Global illumination]]
     - Numerical approximations of the [[https://en.wikipedia.org/wiki/Rendering_equation][rendering equation]]
       - [[https://en.wikipedia.org/wiki/Unbiased_rendering][Unbiased]] vs. biased renderers
     - Path tracing
     - Metropolis Light Transport
     - Photon mapping
     - [[https://en.wikipedia.org/wiki/Radiosity_(computer_graphics)][Radiosity]]
     - [[https://en.wikipedia.org/wiki/Ambient_occlusion][Ambient occlusion]]
   - This is a "hard problem"(tm) and I'm ignoring it here.

 #+BEGIN_NOTES
   - How necessary is this slide?
   - Give real-world example of why this is needed
   - Explain why ray tracing by itself doesn't cover this
   - Show examples of some (e.g. AO)
   - http://www.yafaray.org/documentation/userguide/lightingmethods
 #+END_NOTES


** Modern Day
   - Raytracing is now much more common in movie CGI
   - Intel and NVidia are also pushing it for realtime rendering
   - RenderMan Shading Language is now deprecated
   - Sony Pictures ImageWorks: [[https://github.com/imageworks/OpenShadingLanguage][OSL (Open Shading Language)]]
     - [[http://www.blender.org/][Blender]] implements OSL

